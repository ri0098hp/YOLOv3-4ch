# train
  python train.py --data data/fujinolab-all.yaml --batch-size 12

  # multi gpu training
    python -m torch.distributed.run --nproc_per_node 2 train.py --batch-size 16 --data data/fujinolab-all.yaml --device 0,1
  # with sync batch norm --sync-bn
    python -m torch.distributed.run --nproc_per_node 2 train.py --batch-size 16 --data data/fujinolab-all.yaml --device 0,1
